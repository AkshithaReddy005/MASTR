# PPO Configuration for Fast Training
env:
  id: "MVRP-v0"
  num_customers: [20, 50, 100]  # Curriculum learning steps
  num_vehicles: 5
  vehicle_capacity: 200
  max_time: 1236  # From C101 dataset
  
training:
  total_timesteps: 1_000_000
  n_steps: 2048
  batch_size: 1024
  n_epochs: 10
  gamma: 0.99
  learning_rate: 3e-4
  ent_coef: 0.01
  vf_coef: 0.5
  max_grad_norm: 0.5
  clip_range: 0.2
  clip_range_vf: 0.2
  
model:
  policy: "MlpPolicy"
  net_arch: [256, 256]  # Network architecture
  
logging:
  tensorboard_log: "runs/ppo_mvrp"
  save_freq: 10000
  eval_freq: 10000
  n_eval_episodes: 5
  
seed: 42
